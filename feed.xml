<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://robindavid.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://robindavid.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-03T09:58:12+00:00</updated><id>https://robindavid.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple page, to reference various Robin&apos;s publication all at a single place. </subtitle><entry><title type="html">Our Pwn2Own journey against time and randomness (part 2) - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2023/our-pwn2own-journey-against-time-and-randomness-part-2-quarkslabs-blog/" rel="alternate" type="text/html" title="Our Pwn2Own journey against time and randomness (part 2) - Quarkslab’s blog"/><published>2023-11-07T00:00:00+00:00</published><updated>2023-11-07T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2023/our-pwn2own-journey-against-time-and-randomness-part-2---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2023/our-pwn2own-journey-against-time-and-randomness-part-2-quarkslabs-blog/"><![CDATA[<p>Part 2 of a series about participation in the Pwn2Own Toronto 2023 contest.This blogpost is the second part of the series about our journey to the pwn2own Toronto 2022 contest.In Our Pwn2Own journey against time and randomness, part 1 we explained how we attacked the router from the WAN side and lost our battle against randomness and time just by a few seconds. Here we will describe two vulnerabilities that we found on the LAN side of the Netgear RAX30 router.Pwn2own Toronto 2022 occurred on December 6-9, 2022, so why publish this now ?Well, we reported the discovered vulnerabilities to the vendor and followed a coordinated vulnerability disclosure process that took a lot of time, and we decided not to rage-quit and just publish them, so after a lengthy coordination process we reached the agreed date for publication and here’s the blog post.This vulnerability known as CVE-2023-27368 was not attributed to us, it seems that another competitor also found it. However, since we found this vulnerability during the competition, we will describe it nonetheless.On the LAN side, a server brought to our notice this program called soap_serverd, it is a server that is exposed on all LAN interfaces but does not seem to be designed for human interaction, which is of great interest to us.In fact, this service is used by the Netgear application to communicate with the router.During our analysis, we realized from the first line of parsing the HTTP entry that some things could go wrong. Indeed, as you can see in the decompiled code below, the parsing of the HTTP header is using sscanf with a string format without size control.soap_serverd reads the HTTP headers calling sscanf to extract the HTTP method, path, and protocol version.Because we didn’t want to waste any time unnecessarily, our strategy was to focus on vulnerabilities that were easier to exploit, and we kept this vulnerability for the time we had left. The Netgear hotfix released right before the contest forced us to change our plans and we never went any further than a simple DOS with this vulnerability.However, we were happy to see that, in their last release Netgear corrected this vulnerability by controlling the size of the parsed elements.If you are interested in a blogpost that converts this vulnerability into a remote code execution one, we strongly encourage you to read this blogpost.The Netgear RAX30 router integrates several paths to access a set of services, some with authentication and some without. Among the unauthenticated calls, there is the possibility to push files to the router via the rex_cgi binary. Below, an extract of rex_cgi which handles the file uploads.As you can see, the file is directly written inside the /tmp directory, and no size verification is done. Thus, if you submit a file of a size greater than router’s available RAM, the router will freeze.To try this you can use this script:To have a POC that works properly, you should have a test.cfg file of size over 1G, the RAM size of this router.This vulnerability is not critical, but it could be really inconvenient for anyone wanting to use this router.We reverse engineered the firmware format of the Netgear RAX. The figure below details it precisely:This analysis was based on the original image found on the manufacturer’s website, the bcm_flasher binary and its library libbcm_flashutil.so allowing to flash the image on the NAND memory and the library libpu_util.so containing two interesting functions for the understanding of the firmware format:The router uses a wrapping header for the ITB format used on Broadcom chipset and some other chipset, the ITB format is a firmware format based on FDT (Flattened Device Tree) for SoC (System-on-a-Chip) of the ARM processor family. The ITB format is designed to store a firmware image that includes both the firmware binary code and a Flattened Device Tree representation of the devices embedded in the SoC. This representation allows the firmware to recognize the hardware devices and configure them correctly. The ITB format also allows for the storage of important metadata such as firmware version, security information and firmware integrity checks. It is possible for instance to create a signed image commonly called a FIT image, but for our case it is not used. They have their own method for signing the firmware.In fact, one field is called signature, and this signature is based on sha256 and 2 salted parts (K and K2). This verification is found inside the puUtl_signCfg function.A sha256 hash is computed with hr89sdfgjkehx + PADDING + TYPE_VER + SIZE + VERSION + TYPE_VER + SIZE + DB_VERSION + ITB_HEADER + ITB + nohsli9fjh3f.Thus, because the “signature” of a firmware image is just the SHA256 digest of image data concatenated with a secret string embedded in the firmware it is possible to create firmware images with valid signatures that will pass the signature validation check.During our participation in the Pwn2Own Toronto 2022 contest, we discovered a couple of vulnerabilities that were exploitable on the LAN side of the router. One of them was also discovered by another participating team and it is fixed in the latest firmware version. We reverse engineered the firmware’s format and also discovered that it is possible to create custom firmware with a valid signature and flash it on the router for persistence. We reported the vulnerabilities to Netgear and followed a 173-day coordinated disclosure process for publication.After the intial publication of this blogpost on 7th of November 2023, Jimi Sebree informed us on X that Tenable discovered the same vulnerabilities (Tenable blogpost). These two vulnerabilities are known as CVE-2023-28338 for the denial of service and CVE-2023-28337 for the signature bypass of the whole firmware explained above.If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Part 2 of a series about participation in the Pwn2Own Toronto 2023 contest.]]></summary></entry><entry><title type="html">QBinDiff: A modular diffing toolkit - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2023/qbindiff-a-modular-diffing-toolkit-quarkslabs-blog/" rel="alternate" type="text/html" title="QBinDiff: A modular diffing toolkit - Quarkslab’s blog"/><published>2023-10-12T00:00:00+00:00</published><updated>2023-10-12T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2023/qbindiff-a-modular-diffing-toolkit---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2023/qbindiff-a-modular-diffing-toolkit-quarkslabs-blog/"><![CDATA[<p>This blog post presents an overview of QBinDiff, the Quarkslab binary diffing tool officially released today. It describes its core principles and shows how it works on binaries as well as on general graph matching problems unrelated to IT security.Binary diffing is a specific reverse-engineering task aiming at comparing two binary files that are usually executables. The ultimate refinement of disassembly, baring decompilation, is usually the recovery of functions with bounds along with the associated call graph. As functions represent the different functionalities contained in a binary, they are usually used as the base artifact for diffing. The goal of differs is to compute a mapping between functions of a first binary called primary (A) against those of the second called secondary (B). The mapping computed is usually a 1-to-1 assignment. More evolved approaches try to compute M-to-N assignments as functions can be inlined, or split by means of compilation or obfuscation. This blog post focuses on the 1-to-1 assignment case.Diffing somehow requires comparing the two programs and their functions. So the main question is: which criteria should be used to match two functions? A differ like Diaphora is applying successive comparison passes starting from the most empirically accurate (function names, bytes hashes, etc…). BinDiff, instead, is heavily relying on the call graph and starts from imported functions as anchors. It then explores recursively their call graph neighbors to match functions.While they work very well in most usual cases, they reach some limits on more specific scenarios (e.g.: two banks in the same firmware) or altered binaries like obfuscated ones. Consequently, in the past few years, we explored alternative diffing algorithms that would be more customizable in these scenarios where the reverser might be led to provide their own features and criteria to perform the diff.This led to the development of QBinDiff, a customizable, yet experimental, differ.Formally, we define the graph-matching problem as the process of aligning two attributed directed graphs, where the term align means finding the best mapping between the nodes of the first graph (called primary) to the nodes of the second one (called secondary). In this case what exactly characterizes the best mapping is intentionally left undefined as there are multiple ways of defining what a good match (between two nodes) is. It usually depends on the nature of the underlying problem instance solved. For example, in binary diffing we might consider a match between two functions to be good (aka valuable) if the two functions are semantically equal or similar enough, although, on the other hand, we might also be interested in evaluating how much they syntactically differ, hence, a good alignment has to leverage the similarity between the nodes. In other scenarios, instead, we might be more focused on the topological similarity of the two nodes, which means relying less on the node attributes and more on the call graph (i.e.: graph topology).</p> <p>In the previous image, we can see a representation of the graph alignment problem where we are considering both topological information (the edges) and node attributes (the colors). The black bold arrows represent the alignment (mapping).The graph alignment problem has been analyzed in many research papers1,2,3 and is an APX-hard problem. However, the underlying issue of lacking a unique general definition for a good mapping between the nodes makes it difficult to solve.QBinDiff adopts a unique strategy to combine both domain-specific knowledge and a general theoretical algorithm for graph alignment. It uses two kinds of information:It then uses a state-of-the-art machine learning algorithm based on belief propagation to combine these two information sources, using a tradeoff parameter (α) to weight the importance of each, to compute the approximated final mapping between the two graphs.This approach has the advantage of being versatile so that it can be applied to different instances of the diffing problem, and it leaves the user a lot of space for customizing and tuning the algorithm. Depending on the problem type, some heuristics might be more suitable than others, and sometimes, we might rely more on the graph topology instead of the similarity or vice versa.People not interested in the theoretical aspects of the algorithm can jump straight to the binary diffing section.As we described previously, the similarity between the nodes of the two input graphs is one of the two required inputs for QBinDiff. In practice, this information is encoded as a matrix S that stores at position [n1, n2] the similarity value (between 0 and 1) of the two nodes n1 and n2.In order to keep the most versatility, in QBinDiff, the similarity matrix can either be user supplied (for problem instances that handle attributed graphs) or automatically computed from the binary program by choosing some heuristics from a pre-determined set. These heuristics are called features as they characterize the functions by extracting some information or features.An example of a feature can be counting how many basic blocks there are in the function’s Control Flow Graph (CFG), we can arguably say that similar functions should have more or less the same number of blocks and by using that heuristic (or feature) we can give a score on how similar two functions are. Beware that features are not guaranteed to provide always meaningful results: a feature may be very useful for specific diffing and useless in others. Instead, they should be carefully chosen depending on the characteristics of the binaries being analyzed.To provide the user with even better control over the entire process, it is possible to specify weights for the features, making some of them more important in the final evaluation of the similarity.For the complete list of features look at here. https://diffing.quarkslab.com/qbindiff/doc/source/features.htmlThe second required piece of information is the Call Graph topology similarity between the two graphs. This is equivalent to solving the Network Alignment Problem, i.e. a problem in graph theory where the objective is to find a mapping or correspondence between nodes in two or more networks (graphs) in a way that preserves their structural similarity.The algorithm implemented by QBinDiff to solve the Network Alignment Problem takes into consideration also the similarity matrix obtained before, resulting in a seamless combination of domain-specific knowledge and theoretical approach.The algorithm itself is based on the max-product (or min-sum) belief propagation scheme,4 which, in simpler terms, aims to identify the most probable assignment based on the information gathered so far.Given that the problem is not guaranteed to be solvable in polynomial time (remember that it belongs to the APX-Hard category) a relaxation parameter ϵ must be introduced. This element ensures that the algorithm always operates within polynomial time bounds, but the resulting solution will be an approximation rather than the optimal one. Unfortunately, this is an unavoidable tradeoff that we must accept.This algorithm comes from the work of Elie Mengin. For a complete in-depth description refer to his thesis5 and articles6,7.While QBinDiff has been designed to be use-case agnostic, it has initially been written for binary diffing. As such, it provides all functionalities for extracting attributed graphs from binaries.Similarly to other differs, it relies on existing disassemblers to parse the executable format and to lift machine code into assembly code and high-level structures needed to understand the code (Control Flow Graph, Call Graph, cross-references, etc…). This process can be really complex considering the variety of different instruction set architectures and platforms that exist. So it follows the Unix philosophy that software should do only one job and should do it well, QBinDiff doesn’t disassemble binaries per-se, but relies on the analysis of third-party software (IDA, Ghidra, Binary Ninja, etc…) via exporters.The purpose of an exporter is to serialize the disassembly and all relevant information in a file that can then be processed by other software. Most differs rely on exporters to work. QBinDiff supports the following exporters acting as a backend to load programs.QBinDiff historically used BinExport (like BinDiff), but later integrated Quokka. There are several differences between the exporters, some of them might affect the diffing result in a good or bad way. Below we show a very shallow comparison between the two binary exporters, Quokka and Binexport.*Ghidra extension under developmentOne can read here and here for a more in-depth analysis of binary exporters.These two exporters are integrated into QBinDiff as backends to perform a diff. Their sole purpose is to offer an interface for QBinDiff to interact with the exported analysis made by the disassembler. There is also a backend directly using IDA Pro. In addition to these 3 loaders, it is possible to develop a custom one by implementing a specific interface. See the tutorial for more information.QBinDiff can simply be installed with pip:This will install both the standalone differ and the python library but not the backend loaders. To install them with pip runFor this example let’s consider primary and secondary, the two executables to compare. The disassembly can be exported with BinExport in primary.BinExport and secondary.BinExport respectively.Now the standalone differ can be run like this:Let’s look closely at each option:The result is a BinDiff file that contains the mapping between the functions of the two binaries. In order to visualize the diffing, the file result.BinDiff can be opened with BinDiff.More details are available on the GitHub page: https://github.com/quarkslab/qbindiff.</p> <p>Example of BinDiff UI visualizing the QBinDiff generated result.BinDiffQBinDiff was designed particularly to be used as a library for programmatic diffing. The previous example can be reproduced with the Python snippet below.After completing the diff, one can already play with the result without having to export it to a BinDiff file format, in fact, it is possible to access the Mapping object that contains the entire detailed mapping between functions with the similarity and confidence scores.The result that we obtain should be pretty much the same as before.As shown above, QBinDiff is solving an optimization problem that goes beyond binary diffing. In fact, binary diffing is somehow just an instance of this problem. Different research fields, especially biology, also have this kind of problem.We designed a low-level API that works on any kind of problem given the two core elements of the algorithm:In binary diffing, these base objects are functions, but they can be pages/profiles in social networks, or atoms in molecules analysis.One important use case in bioinformatics is the alignment of protein-protein interaction (PPI) networks of different species8. A PPI network is a huge graph in which the nodes are proteins and edges represent interactions between them. A comparative study of these two graphs can reveal important insights that may help in disease analysis, drug design, understanding the biological systems of different species, and more.In this example, we are going to analyze the PPI networks of Homo sapiens (human) and Mus musculus (mouse). These networks have been studied multiple times and as such are available in several open databases. In this case, we used the BioGRID public database, which archives and disseminates genetic and protein interaction data collected from over 70,000+ publications in primary literature.</p> <p>Visualization of the PPI networks. On the left is displayed the Homo sapiens while on the right is the one of the Mus musculus. The colors help to identify graph communitiesIn order to provide a similarity matrix between the nodes (in this case the proteins) of the two graphs we can use the BLAST algorithm,9 that computes the similarity between two proteins by comparing their amino-acid sequences.For the sake of clarity suppose that we already have Python objects representing the graphs and the similarity between the nodes.Now that we have all the data we need, it’s time to create a Differ instance and compute the alignment of the two networks. Since we are working with undirected, unattributed graphs we can use the GraphDiffer class from qbindiff.To load the similarity matrix with our scores, we can register a Pass function. The pass mechanism is used for refining the similarity matrix. To populate the similarity matrix with the BLAST scores we can use the pass mechanism, a callback system that is used for refining the similarity matrix at each pass. By default a GraphDiffer instance will initialize the entire similarity matrix to 1, hence it’s not leveraging similarity information, so we can re-initialize it to 0 and then put the normalized similarity BLAST score.At this point it’s easy to manipulate the Mapping object containing the alignment of the two PPI networks.The complete script as well with the dataset can be downloaded here.As part of our work on diffing, we tried to aggregate various resources and documentation about the various tools on a single web page: the diffing portal. It also contains in-depth explanations about the algorithm used by QBinDiff and other differs as well as tutorials and quick start guides, academic papers, and documentation about binary exporters and differs.It can be reached at this link https://diffing.quarkslab.com/.We open-sourced QBinDiff, an experimental tool that requires some know-how to get the most of it. However, it’s a good platform for experimentation and more specific diffing tasks. Because of its implementation in Python, it never will be faster than Bindiff but it does not intend to :).Multiple experiments are in the pipe especially to compare it against Diaphora3 and its features from the decompiled code. Also, more academic results are hopefully coming soon!The goal of this post was to give an insight into QBinDiff’s algorithm and how diffing can be applied beyond reverse-engineering. We are also eager to receive constructive feedbacks on our tools. To conclude, if you have other use cases where such approach can be useful, let us know!Burkard, Rainer E. (Mar. 1984). Quadratic assignment problems. European Journal of Operational Research 15.3, pp 283-289. ↩Bayati, Mohsen et al. (Dec. 2009). Algorithms for Large, Sparse Network Alignment Problems. Proceedings of the 2009 Ninth IEEE International Conference on Data Mining. ICDM ‘09 USA: IEEE Computer Society, pp, 705-710. ↩Klau, Gunnar W. (Jan. 2009). A new graph-based method for pairwise global network alignment. BMC Bioinformatics 10.1, S59. ↩Loeliger, H.-A. (Jan. 2004). An introduction to factor graphs. IEEE Signal Processing Magazine 21.1, pp. 28–41. ↩https://theses.hal.science/tel-03667920/ ↩Elie Mengin, Fabrice Rossi. Improved Algorithm for the Network Alignment Problem with Application to Binary Diffing. 25th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES2021), Aug 2021, Szczecin, Poland. pp.961-970. https://arxiv.org/abs/2112.15336v1 ↩Elie Mengin, Fabrice Rossi. Binary Diffing as a Network Alignment Problem via Belief Propagation. 36th IEEE/ACM International Conference on Automated Software Engineering (ASE 2021), IEEE; ACM, Nov 2021, Melbourne, Australia. https://arxiv.org/abs/2112.15337 ↩Titeca, Kevin; Lemmens, Irma; Tavernier, Jan; Eyckerman, Sven (29 June 2018). Discovering cellular protein‐protein interactions: Technological strategies and opportunities. Mass Spectrometry Reviews. 38 (1): 79–111. ↩Stephen F. Altschul, Warren Gish, Webb Miller, Eugene W. Myers, and David J. Lipman. Basic Local Alignment Search Tool. Journal of Molecular Biology. 1990: 215(3) ↩If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post presents an overview of QBinDiff, the Quarkslab binary diffing tool officially released today. It describes its core principles and shows how it works on binaries as well as on general graph matching problems unrelated to IT security.]]></summary></entry><entry><title type="html">PASTIS For The Win! - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2023/pastis-for-the-win-quarkslabs-blog/" rel="alternate" type="text/html" title="PASTIS For The Win! - Quarkslab’s blog"/><published>2023-05-17T00:00:00+00:00</published><updated>2023-05-17T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2023/pastis-for-the-win---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2023/pastis-for-the-win-quarkslabs-blog/"><![CDATA[<p>In this blog post we present PASTIS, a Python framework for ensemble fuzzing, developed at Quarkslab.PASTIS is an open-source fuzzing framework that aims at combining various software testing techniques within the same workflow to perform collaborative fuzzing, also known as ensemble fuzzing. At the moment it supports Honggfuzz and AFL++ for grey-box fuzzers and TritonDSE for white-box fuzzers. The following video (in french with english subtitles) gives an insight into the principles of PASTIS:</p> <p>In May 2023 PASTIS participated in a fuzzer competition sponsored by Google in the context of the 16th International Workshop on Search-Based and Fuzz Testing (SBFT) co-located with ICSE 2023, the 45th International Conference on Software Engineering, one of the longest running and most prestigious software engineering venues.Our collaborative fuzzing approach won first place, tied with aflrustrust, in the bug discovery category which ranks the fuzzers that find the highest number of unique bugs. The paper, published in the research track of the workshop, presents the contributions of this work:</p> <p>PASTIS is now open-sourced under Apache License 2.0. You can find it on the Github repository.In this blog post we present an overview of the framework and a simple guide to start using it in your projects.Software testing is crucial to uncover bugs and vulnerabilities. To that end, multiple automated testing techniques like fuzzing are used. This approach has been extensively studied in the literature and improved over the last few years. Fuzzing relies on executing as many iterations as possible of a target program over different inputs generated with pseudo-random mutations and possibly with the help of a structure model or grammar. Both execution and input generation algorithms have been improved over time to explore deeper program states.Dynamic Symbolic Execution (DSE) is another approach to software testing. It is a formal technique also used for program exploration and testing. Advances performed in this research area made it a functional approach used in state-of-the-art software testing tools. The DSE principle is to precisely model each instruction’s side-effects to track input propagation in the program and express branching conditions as first-order logic formulas.While fuzzing is empirically effective, it tends to cover shallower states. In comparison, DSE is slower but is theoretically able to cover deeper states by solving complex branch conditions or complex code constructs.The goal is to combine grey-box fuzzing and DSE to leverage their respective strengths and reach better coverage than either of these approaches on its own, or at least, obtain the same coverage faster. Challenges are threefold. First, one needs to deal with the implementation discrepancies of various engines, such as input formats and execution speed. Second, input generation throughput is a challenge as input flooding might alter the normal behavior of engines. The last challenge is to combine them asynchronously so that no one is blocking or slowing down the others.We propose a combination of fuzzing and DSE into an ensemble fuzzing framework called PASTIS that helps in circumventing engines inner-working discrepancies.Our approach combines heterogeneous test engines by solely sharing test cases (inputs). Each engine then decides whether to drop it or not. If the input triggers a new program behavior regarding a given engine’s coverage metric the input is kept, otherwise it is discarded. Being significantly slower than fuzzing, DSE should replay each input it receives at a satisfying speed to update its coverage and decide whether to keep the input. We designed an ensemble fuzzer combining grey-box fuzzing and white-box fuzzing (DSE) built around a broker that performs seed sharing and aggregates the resulting corpus and data.PASTIS benefits from Honggfuzz and AFL++ two widely-used and effective grey-box fuzzers. PASTIS also takes advantage of TritonDSE, our Python framework for dynamic symbolic execution released recently.PASTIS is composed of two main components: a broker and a set of engines or fuzzing agents.The broker, called pastis-broker, is the main interface with the user. It is implemented in Python and ensures all communications between the available engines. It is built using a library called libpastis which handles all the communications.The communication protocol is based on the message-queuing framework ZMQ, which is interoperable with almost all existing programming languages. However, the most interesting feature it provides is over-the-network communication. This allows PASTIS to be run over multiple machines.An engine in PASTIS is any fuzzer or DSE tool wrapped in a thin Python module, called Driver (also built using libpastis). This module implements a series of callbacks that allow communication with the broker. The broker sends the engines the target, settings, and seeds. The engines, on the other hand, send the generated inputs and telemetry. Each engine handles coverage using its metric, adding or discarding an incoming seed according to its own rules. This approach allows sharing of seeds easily. The broker is in charge of aggregating the inputs produced by the engines and sharing them.The three fuzzing engines supported right now are Honggfuzz, AFL++, and TritonDSE (pastis-honggfuzz, pastis-aflpp, and pastis-tritondse, respectively). PASTIS implements a driver for each fuzzer.The figure below summarizes the architecture of PASTIS. It shows the main interactions between the fuzzers and their respective wrappers. All inter-communications are performed through filesystem monitoring (inotify on Linux).The FSM demo is a tiny software implementing a state machine that contains a bug. It shows how to combine the various approaches into a collaborative fuzzing campaign within the PASTIS framework.The code fsm.c read “packets” from stdin. Each packet is a struct composed of an ID (16 bits) and a data integer (32 bits). Depending on the ID and the data the FSM switches states. You can download it from hereAfter installing PASTIS, we need to build our target. For this example, we only have to run make. Keep in mind that the target is compiled using the compilers provided by Honggfuzz and AFL++, hfuzz-clang and afl-clang, respectively. This will instrument the target for both fuzzers. This is not necessary for TritonDSE as it processes the target binary without any instrumentation. Below we show the commands to do this:After compilation, it is just a matter of launching the broker and each engine. Note that the broker receives three parameters. The first one points to the folder with the three versions of the target binary. The second one points to the folder with the initial corpus. The last one points to the workspace used by PASTIS, where it will save new inputs, crashes, hangs, logs, and stats.By default, PASTIS shares the generated inputs with all the running engines. That is, the input generated by one engine is added to the corpus of the other engines. Depending on the target this can be beneficial or not. This can be changed using the –mode option.Once the broker starts running, you’ll see the below output on your screen, which indicates that it detected all three binaries.The broker will wait until, at least, one engine connects. To launch the engines is just a matter of running three commands (in three different shell sessions):After a few seconds, all the engines are connected to the broker and working as shown in the screenshot below (the broker in the left):It is worth noting that PASTIS can run on different machines. This means that the broker as well as each engine can run on a different machine. For those interested in trying, it’s just a matter of adding the command-line option –host <IP-OF-THE-BROKER> to each engine (it's possible to specify the port with --port <PORT>, the default one is 5555). For example, the AFL++ engine the commands would be: pastis-aflpp online --host <IP-OF-THE-BROKER>.We also provide a docker image, for those who want to try it without installing the dependencies. You can find it herePASTIS is documented, here you will find how to install it and run it, a demo and the Python API. The documentation also includes instructions on how to add a new fuzzer.This blog post presented PASTIS v0.1.1, a Python framework for ensemble fuzzing. PASTIS is one of the many projects developed at Quarkslab as part of our efforts to improve and ease our daily tasks on binary analysis and vulnerability research. We are now glad to open-source it so others can benefit from it.The framework is experimental, any valuable feedback or contributions are greatly appreciated!We would like to thank DGA-MI that initially funded this work. We also want to warmly thank all past contributors of the project, Acid, djo and Richard.If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</IP-OF-THE-BROKER></PORT></IP-OF-THE-BROKER></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this blog post we present PASTIS, a Python framework for ensemble fuzzing, developed at Quarkslab.]]></summary></entry><entry><title type="html">Introducing TritonDSE: A framework for dynamic symbolic execution in Python - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2023/introducing-tritondse-a-framework-for-dynamic-symbolic-execution-in-python-quarkslabs-blog/" rel="alternate" type="text/html" title="Introducing TritonDSE: A framework for dynamic symbolic execution in Python - Quarkslab’s blog"/><published>2023-05-02T00:00:00+00:00</published><updated>2023-05-02T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2023/introducing-tritondse-a-framework-for-dynamic-symbolic-execution-in-python---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2023/introducing-tritondse-a-framework-for-dynamic-symbolic-execution-in-python-quarkslabs-blog/"><![CDATA[<p>We present TritonDSE, a new tool by Quarkslab. TritonDSE is a Python library, built on top of Triton, that provides easy and customizable Dynamic Symbolic Execution capabilities for binary programs.TritonDSE is a Python library built atop the existing Dynamic Symbolic Execution(DSE) framework Triton to provide more high-level program exploration and analysis primitives. The whole exploration can be instrumented using a hook mechanism that allows the user to run custom code on various events, like address, mnemonic, new input generated, each iteration, a branch to be solved, etc. It can be seen as a symbolic unicorn-like framework as it is not an off-the-shelf program, but a toolkit to build dedicated and specific analyses. Still, it is able to perform some exploration on its own and provides ways to customize it. It was partly designed to build a whitebox fuzzer now integrated into PASTIS. The framework is still experimental, thus any feedback or issue reports are appreciated.Why not use Triton directly?Triton is a DSE library providing all the necessary elements to analyze traces with concrete or symbolic information and also to generate and solve path constraints. It is written in C++ (Core and API) and it has bindings for Python. It works on all the major operating systems and supports the main architectures: x86, x86_64, ARM v7, and ARM v8. Yet, it is a low-level library. This means that it provides its users with all the required components to perform DSE tasks, however, it is the user who has to take care of the rest. That is, to load the binary in memory, load shared libraries, handle syscalls and more especially feed every instruction to execute symbolically to the engine. This can be a lot of work.TritonDSE tries to address all these problems and adds extra functionality such as program exploration capabilities right out of the box. It works by performing an elementary loading of a given program and starting to explore it from its entry point. At the moment solely ELF and Linux are supported, but further development can lead to the support of more platforms.TritonDSE provides the following features:TritonDSE is now open-sourced under Apache License 2.0. You can find it on the Github repository.TritonDSE allows users to load a full binary and start analyzing it right away. That means it is ready to be run (emulated through Triton) from its entry point, or any other address set by the user. It is possible to add hooks on many different events, such as when a given address is hit, on a given mnemonic, on memory accesses, and so on. This allows for a quick analysis of the program in just a few lines of Python.It is possible to load a raw binary as well, i.e. a binary without a format, such as the case of firmware. In this case, users can manually describe the different sections a given firmware has, where they start and finish, and even set permissions for them.TritonDSE comes with a memory segmentation feature that allows to set permissions, such as Read, Write and Execute, on memory regions. These are directly loaded from the binary, however, they can also be set manually.TritonDSE also provides a probe mechanism that enables the attachment of modules during the exploration process. These modules can hook various events, allowing the user to implement, for instance, custom sanitizers.The most interesting feature that TritonDSE provides is its program exploration capabilities. Under this use case, users load the target binary and provide a set of initial seeds. TritonDSE will use these seeds to run the program, collect path constraints during the execution, and generate new inputs. Each input corresponds to a branch condition that was not taken in the parent input. For instance, let’s suppose we start with only one seed. When we run the program using this seed as input, the program will manipulate the bytes from the input and take decisions based on them. That is, it will make checks using if statements, and depending on the result, it will take the then or the else branch. TritonDSE collects all those branches and negates them to generate an input that exercises the opposite direction (if in the original input, a then branch was taken, in the derived seed generated by TritonDSE the else branch will be taken). There will be branches for which it is not feasible to yield the opposite result due to contradictory restrictions. This way, and by repeating this process (that is, retro-feeding the newly generated inputs) TritonDSE can explore a program. Therefore, you can use TritonDSE to explore a program to help you in your vulnerability research tasks. You can combine this exploration with classic fuzzing tools, such as AFL++ and Honggfuzz, to improve your results.Moreover, TritonDSE implements different coverage strategies, like Block, Edge, or Path. These strategies allow the user to customize the exploration, providing a balance between accuracy and speed. Block is the most basic coverage strategy. A basic block is considered covered simply if it is executed (that is, if TritonDSE manages to generate an input that exercises that particular basic block). On the other hand, Edge considers both the source and destination of a branch. Therefore, if a basic block can be reached from multiple locations, it will be marked as covered only when all pairs of source-destination were covered. Finally, Path considers all the possible ways to get to a given point in a program and this point will be considered covered when all of them have been executed.To summarize, TritonDSE not only provides great binary program analysis capabilities right away, but it is also designed to be highly customizable and easy to use.Let’s use a simple crackme, shown below, to display TritonDSE’s basic program exploration features:This program receives input from the command line through argv. When provided with the correct input, it will display Win.To automatically solve this crackme, we use the following script:This script will execute the target symbolically starting with AAAAAAAAAAAAAAA as input. It will collect the branches that depend on the input, invert them, and produce a new input, which will be added to the corpus. It will repeat this process until it can no longer yield an input that covers new code.The code is straightforward. It loads the program and sets the configuration for the SymbolicExplorator. Then, it creates a seed and adds it to the corpus. There are two types of seeds: Composite and Raw. The first allows the user to fine-tune the input to inject. In this case, it allows the specification of the value of argv (it can also be used to specify files and variables). The Raw format, as expected, is just a sequence of bytes that are directly passed to the program (useful in cases where the program reads from stdin). Notice that we also make use of the hooking mechanism. Here we use it to display the seed hash and its content just before the program starts (you can read more about hooks here). Another point to notice is that we have not set up a hook on printf, TritonDSE does it for us, as it comes with support for basic libc functions.The following is a snippet of the output. Notice the two new inputs generated (using the Z3 SMT solver).A few lines below we can see how it generates the input that solves the crackme:This was just a simple example of how to load and explore a program very intuitively and in just a couple of lines of code. TritonDSE can load and handle complex binaries and handle x86/x86_64 and ARM32 architectures. Currently, it is used a whitebox fuzzer integrated into PASTIS.TritonDSE is well documented, here you will find how to get started, the basic Python API and the advanced one, and even exercises that will let you get familiar with its concepts, which type of problems can be solved and how to solve them. There are Jupyter Notebooks as well.In this blog post, we presented TritonDSE v0.1.2, a Python library providing exploration capabilities for binary programs. This is one of the many projects that we developed in Quarkslab as part of our efforts to improve and ease our daily tasks on binary analysis and vulnerability research. We are now glad to open-source it so others can benefit from it as well.Stay tuned for more news on TritonDSE!If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We present TritonDSE, a new tool by Quarkslab. TritonDSE is a Python library, built on top of Triton, that provides easy and customizable Dynamic Symbolic Execution capabilities for binary programs.]]></summary></entry><entry><title type="html">Our Pwn2Own journey against time and randomness (part 1) - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2023/our-pwn2own-journey-against-time-and-randomness-part-1-quarkslabs-blog/" rel="alternate" type="text/html" title="Our Pwn2Own journey against time and randomness (part 1) - Quarkslab’s blog"/><published>2023-03-24T00:00:00+00:00</published><updated>2023-03-24T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2023/our-pwn2own-journey-against-time-and-randomness-part-1---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2023/our-pwn2own-journey-against-time-and-randomness-part-1-quarkslabs-blog/"><![CDATA[<p>A journey into the Pwn2Own contest. Part 1: Netgear RAX30 router WAN vulnerabilitiesQuarkslab participated in Pwn2own Toronto 2022 in the router category. This blog post series describes how we selected our targets, performed our vulnerability research, and goes over our findings on the Netgear RAX30 router. The first blog post focuses on our vulnerability research on the RAX30 WAN interface, while the second part will detail the research performed on the router’s LAN.Disclaimer: All vulnerabilities shown in this blog post have been reported to Netgear.Pwn2Own is a worldwide contest that includes multiple targets such as automotive, routers, domotic equipment, and operating system, among others. The goal is to exploit these devices and gain root access in less than five minutes.We participated in the router category with a team of 3 members:To go deeper into the rules for local devices, only one type of vulnerability was accepted: remote code execution. For the routers category, there were two possible vectors: via LAN and WAN, with a higher cash prize for the latter.The WAN (Wide Area Network) is the side connected to the Internet, which means that anyone from around the world can access it. In comparison, the LAN (Local Area Network) can only be accessed by devices directly connected to the router, and these devices can communicate with other devices connected through the LAN.For this research, we took about a month to choose our targets, which seems long in retrospect. To prepare Pwn2own, what seemed the most important to us was to establish a scrupulous methodology that would allow us to check a large number of entrypoints/vulnerabilities, probably in a first step without buying the hardware, based on the firmware files available for download on the vendor website. Once we have a valid methodology we can automatize as many tasks as possible and run these automated tests again a set of targets, thus optimizing our efficiency.As you can see on the above timeline, many vulnerabilities have been found in a very short time but most of them were fixed by Netgear’s hotfix. Vulnerabilities that are easy to find, also known as “low-hanging fruits”, are the first ones to be found by competitors as well as the first ones to be fixed by the vendor, but still can be very useful in some attack scenarios. Therefore, we cannot simply rely on this kind of vulnerabilities to compete at Pwn2own because there is a high probability that other teams may find them too, but also that Netgear fixes them just before the contest starts (spoiler alert: they did).Complex vulnerabilities however can be difficult to trigger and exploit, and the time required to develop a reliable exploit may impact our capability to craft an attack scenario based on them. Especially when you start your vulnerability research a few weeks before the contest. So we may have no choice but to find both types of vulnerabilities, and cross our fingers the device would be still vulnerable on D-day.As part of our daily work, we analyze plenty of firmware files. This task is time-consuming given the variety of manufacturer and firmware formats. Quarkslab has built a framework called Pandora to help with this task. This framework helps extract the filesystem and analyze it by providing a programmatic API to automatize some analyses. It notably enables comparing some firmware versions to extract key differences.To make the analysis as efficient as possible in a short time, we have chosen to focus on one possible attack. In the context of Pwn2own, the scope was restricted to attacking the target WAN interface. Moreover, no service was exposed on the WAN side restricting, even more, the potential attack surface of the device. Furthermore, the router can be potentially disconnected from the internet without any device connected to it. Thus, a well-known remaining attack vector is DNS spoofing which allows MITM attacks to be performed.If successful, a DNS spoofing attack replaces an IP address reached by the router with the attacker’s one. For instance, if the router queries the IP address of dns.google.com that normally resolves to 8.8.8.8, a malicious user present on the same network may intercept this query and send a spoofed answer telling the router this name resolves to 10.10.0.1, an IP address corresponding to another machine present on the same network. The router then may connect to a rogue system and access what seems to be a legitimate service, while it is owned by an attacker and can be used to perform multiple attacks.Nowadays most servers use authenticated and encrypted protocols like HTTPS to prevent these attacks. As expected, most of the URLs used by this router rely on HTTPS. However, even though a certificate is associated with a server to prove its identity, the client still has to check it. For instance, curl ensures the remote server presents a valid certificate unless provided with the -k option which skips this check:As the certificate is not verified, an attacker in a MiTM position can present a self-signed one and it will go unnoticed by the client that will not realize that the service it is requesting is malicious.To identify this weakness in executable files, we need to identify not only calls to curl with the -k option but also all calls to the libcurl library with the appropriate options. Fortunately, curl has an option to generate a code snippet of the command line call, by running:The resulting file test.c contains the following code snippet:In particular, the 2 following options are of interest:They are set to 0 when the -k option is enabled. Thus, we just need to identify all the binaries present in the firmware calling some specific functions exported by libcurl with these options set to 0.Using Pandora, we listed the binaries importing libcurl using the lief analysis. Then we wrote an IDA script to analyze the options used by these calls and finally, we identified a list of vulnerable binaries:We also noticed that the libfwcheck.so library uses insecure HTTPS requests through libcurl. The faulty code is located in the fw_check_api function of libfwcheck.so. This function updates the endpoint base URL to download firmware and uses libcurl to query a remote web service with the following JSON content:When looking at the options provided to libcurl, we identified the flag number 64 (CURLOPT_SSL_VERIFYPEER) and 81 (CURLOPT_SSL_VERIFYHOST), both set to 0. As explained before, these parameters allow an attacker to perform a MITM attack and spoof the target servers.Thanks to that function, we could impersonate the server through DNS spoofing and extract the target model, serial, and current firmware version. We then can send back an answer that will be processed by the router. Let’s have a look at the code in charge of parsing this answer, located in the fw_check_api function.The parser uses the cJson library to process the response sent by the server, which is made of 2 elements: a status and a specific URL. The string url is then copied only if the status is 1. Interestingly, we can now change the URL written in this file, but we do not know where it is being used yet.To achieve that, one can list all the binaries using this function. We focused on the pucfu binary, as shown below:The binary calls the get_check_fw command and then stores the URL in the /tmp/fw/cfu_url_cache file.After a quick research, we identified two executable files that access this specific cache file:When we analyzed the puraUpdate and pufwUpgrade binaries, we identified a function called DownloadFiles that is called with the URL read from the cfu_url_cache file.The following decompiled code shows how the URL read from this file is used to generate a system command using curl, in the DownloadFiles function:The url parameter is directly used inside this command line that would be executed later as a shell command. So if we provide an URL like the following:The final command executed would be the following:In summary, we can impersonate a specific web service using DNS spoofing from WAN side, and force the router to connect to our rogue server that returns a specifically-crafted URL that then is stored in a temporary file. This same temporary file is then used by another program to access our rogue web service with certificate validation enabled (that fails), but also another curl command that has been injected in the temporary file. This command triggers the download of a malicious shell script that is then executed on the target system, achieving remote code execution.Sadly, Netgear issued a hotfix the day before the submission deadline. The latest firmware version now uses execve() to avoid this nasty shell escape vulnerability:The parameter url is now understood by the system as a parameter in its whole and not part of the shell command line anymore.If we take a bigger picture of this DownloadFiles function, we can see that certificates are only verified when the provided URL begins with https. But as seen previously, we can downgrade from HTTPS to HTTP, thus evading the certificate check:Let’s have a look at how puraUpdate communicates with the original server and how we can replicate the servers’ behavior:Below is the content of our fileinfo.txt file:puraUpdate queries this server, retrieves the fileinfo.txt file, downloads the corresponding binary file, and stores it in its filesystem and executes it.The exploitation scheme is detailed in the figure below:After the publication of Netgear’s hotfix, this vulnerability was no longer present in the latest version of RAX30 firmware. Indeed, the pucfu executable was not called anymore and was replaced by a hardcoded URL, thus avoiding any downgrade to HTTP and enforcing certificates checks.Our last chance of successful exploitation resided in the pufwUpgrade binary that was prone to the same HTTP downgrade vulnerability but that was not patched by Netgear.This binary calls pucfu (making sure that the certificates are valid) and then updates the whole firmware.pufwUpgrade can be called with different parameters, especially the following:Let’s focus on how pufwUpgrade -A works:This version of fileinfo.txt differs from the one described above in this post, as shown below:This file contains the md5 hash and the size of the firmware to be downloaded, as well as some messages to give details on the changes included in the new version. It also contains a language table used for web UI localization.When the router boots up, the pufwUpgrade -s command is called to schedule a system upgrade check in the future.This scheduled task is planned at a random time, decided by the previous call to pufwUpgrade. Since this router has no real-time clock (RTC), it relies on an NTP server to synchronize its date and time once it can access the Internet. At boot time, the system date and time are set to the build time of the kernel and then the system starts its NTP client to query the current date and time. However, if no NTP server answers during boot, the system keeps its current date and time which is easily predictable. At the same time, it is difficult to guess exactly when a program is launched, since some tasks and processes may vary.We didn’t succeed in finding any network marker such as a specific packet or request to determine the time seed used to generate the random, so we stuck to the most probable value: 3h00. This is the value we observed the most after numerous reboot attempts.To force the router to trigger a firmware update as soon as possible (remember, we only have 5 minutes to exploit this device), we decided to trick the router into probing a fake NTP server that would provide the expected time and force the upgrade program to be launched. To achieve this, we focused on ntpd, an executable file found in the firmware that seems to be in charge of handling NTP synchronization. In particular, we had to understand what is the best way to force a time update.First of all, ntpd contacts the NTP servers time-h.netgear.com and time-g.netgear.com using the NTP protocol, giving us the possibility to spoof this server and manipulate the date and time… but not the timezone. If none of these servers answer, it launches the ATS executable which requests an HTTPS server to get the current timestamp and the timezone for this router, a sort of recovery plan in case Netgear’s NTP servers are down, presumably.This server is called without any certificate verification as explained before and therefore can be easily spoofed to provide this binary with specifically-crafted values. We emulated this server and returned some values in a way the router updates its date and time to a few seconds before the system update check is started:This successfully triggers the system update mechanism, which in turn checks our rogue server for a new firmware image to be deployed!So far so good, we now have a way to force the router to launch a system upgrade at will with a minimal delay between its boot and the installation of a new firmware. Well, we need a new firmware image now.The fimware is a FIT image with a proprietary header that is used by bcm_flasher to deploy it into the router’s non-volatile memory. FIT images are quite easy to parse and modify since U-Boot comes with some dedicated tools.The proprietary header is pretty simple, it contains the version, the db_version, the board ID, and a signature. We won’t cover in this first part how the signature is computed and checked because pufwUpgrade does not care at all. This signature can be left empty or with some invalid value, the upgrade process will succeed anyway.The following schema summarizes our exploitation scenario:The problem is this attack scenario takes about 4 minutes and a half to complete, including a reboot of the router. We only have 5 minutes to successfully exploit this vulnerability, that is a pretty tight schedule! We decided to implement a LED show that will be launched in the early boot stage, to avoid losing too much time and missing the 5-minute deadline.The following video and animation demonstrate the exploitation scenario and will make you feel as anxious as we were during our exploitation attempts!In the end, we attempted this exploitation scenario three times without being able to successfully compromise the router we had access to during Pwn2own… We had a lot of stress but despite all of our efforts, randomness won this time.If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A journey into the Pwn2Own contest. Part 1: Netgear RAX30 router WAN vulnerabilities]]></summary></entry><entry><title type="html">A Brief Overview of Auditing XCMv2 - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2022/a-brief-overview-of-auditing-xcmv2-quarkslabs-blog/" rel="alternate" type="text/html" title="A Brief Overview of Auditing XCMv2 - Quarkslab’s blog"/><published>2022-03-29T00:00:00+00:00</published><updated>2022-03-29T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2022/a-brief-overview-of-auditing-xcmv2---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2022/a-brief-overview-of-auditing-xcmv2-quarkslabs-blog/"><![CDATA[<p>Parity Tech mandated Quarkslab to audit XCM version 2 (XCMv2), a cross consensus communication mechanism. This messaging protocol is a cornerstone of the Polkadot ecosystem as it enables communications between chains on a network. This blog post summarizes few security aspects related to this technology and its implementation. The full audit report is available in PDF format at the end of this article.Parity Tech is actively working on developments related to the Polkadot ecosystem. Polkadot recently launched the crowdloan process as part of the long-planned objective of creating a network of blockchains interconnected via a relay chain to perform cross-chain exchanges.In this context, Parity is working on XCM (Cross-Consensus Messaging) which provides a common format enabling Substrate-based blockchains to communicate with the relay chain and between each other. This component from the original Polkadot design is essential for Polkadot’s multichain network. It enables both fungible and non-fungible asset transfers and also remote extrinsic calls.Bridges and cross-chain technologies, in general, will play an essential role in the upcoming months or years to interconnect blockchains that are not necessarily working with the same technology or consensus rules. Among these are Hop for EVM blockchains, Interlay to bridge Bitcoin to Polkadot or Axelar Network, which aims at bridging multiple blockchain technologies.XCM follows a similar goal and aims at bridging any kind of consensus in the future. Polkadot’s founder Gavin Wood provides insights of XCM goals as “a language communicating ideas between consensus systems” [1].Quarkslab conducted an audit of XCMv2 before parachains obtained a slot on the Polkadot relay chain and thus before the activation of the support in their blockchain. An additional security audit had already been performed by another security company.The audit aimed at finding any cross-chain-related security issues, like incorrect lock/unlock or burn/mint on both chains, or any fairness issues between chains. This also includes logical bugs, denial-of-service and any misconfiguration (of default settings) that can have a security impact. The audit did not reveal any meaningful security-related issues.This blog post aims at providing a glimpse of the internal working of XCM transactions and more especially the VM-based design for processing messages. It also highlights the key security and sanity checks to be performed before activating XCM on a parachain.The XCM design and the two main use-cases — reserve transfer assets and teleport assets — have thoroughly been described by Gavin Wood in a blog post trilogy [1], [2], [3] and a workshop by Shawn Tabrizi [7]. Moonbeam also provides multiple educational contents about XCM and their usage with XC-20 tokens [4], [5].While user applications of XCM (assets transfers, etc.) have been well described, let us focus on what’s happening under the hood when transferring assets between two chains. The core component is the XCM virtual machine (XCVM). Indeed, messages exchanged are scale-encoded [6] instruction opcodes. A message is essentially a list of instructions that perform various actions like withdrawing assets from an account, depositing them, initiating a transfer, etc.When receiving a new message, a new VM is instantiated for the lifetime of the message execution. Some instructions update registers of the VM. These registers hold the origin (identity performing the action), holding (assets manipulated), and a trader handling weight costs, surplus, and refunded amounts. The VM also contains an instruction pointer addressing the current message instruction to execute. Similarly, it contains registers pointing to error handlers and appendix handlers.Let’s consider a teleport asset transfer from the relay chain to a parachain. In this scenario, the user withdraws funds from its local account and transfers them to the XCVM, which will eventually burn the asset. Then, the chain has to transmit a message to the remote chain to mint and deposit the equivalent amount of assets into the user’s account on the parachain. The complete initial message is the following:The following animation shows the broad steps performed by the XCM pallet teleport_assets call, its processing by the XCM executor and its underlying XCVM, up to the final deposit on the remote account (here Alice).Note that in this scenario, the destination chains have to accept the originating chain as a teleporter, i.e., an origin from which the receiver can trust that the sender rightfully destroyed assets before teleporting them, in its own XCM configuration. Otherwise, the execution of ReceiveTeleportedAsset on the destination chain will reject the message as the origin register will be untrusted. That is one of the reasons why properly configuring XCM is a critical task. Let’s discuss a few aspects of the XCM configuration.Parachain developers do not necessarily need to understand the deep intricacies of the XCM executor, but the pallet configuration leaves many levers of configuration that should be handled with care. Indeed, some configuration aspects are interdependent as highlighted in the report. Here is a quick memo of things to verify before activating XCM or accepting exchanging messages with a chain.In the following list, elements of the XCM pallet and the XCM executor configuration are mixed, in fact, the XCM executor configuration is nested as a configuration element of the XCM pallet. They are marked with the “pallet” or the “executor” (or “both”) annotation.Three filters, XcmExecuteFilter, XcmTeleportFilter and XcmReserveTransferFilter are the first configuration types to look at. These filters are an assertion mechanism respectively for the execute, teleport and reserve_transfer extrinsics (and their limit additions). These types implement the Rust trait (or interface) Contains<T> that just requires a simple method returning true if this "contains" the input value. Two basic implementations, Everything and Nothing, return respectively true and false, and were previously called AllowAll and DenyAll. These filters are a simple way to enable or disable the core features of the XCM pallet, which are the most useful extrinsics, to execute or send messages via privileged execution allowed by the teleport and reserve_transfer extrinsics.ExecuteXcmOrigin and SendXcmOrigin ensure which origin can respectively perform the execute and send extrinsics. They must implement the EnsureOrigin<OuterOrigin> trait. Typically, in the Polkadot or Kusama runtimes, a LocalOriginToLocation tuple composed of many converters transforms Origin into a MultiLocation and thus allows the origin or not. For example, the SignedToAccountId32 implementation transforms a signed origin into the MultiLocation corresponding to the specific account on the chain. Note that execute has a filter, and an XCM origin converter.One of the settings to look at and verify in the executor is the Barrier. This is effectively the mechanism for the executor to filter messages in terms of payment for the execution. Complex logic can be implemented to allow free execution for certain kinds of messages and require payment for others. The Barrier type is usually a tuple of multiple structures that implements the ShouldExecute trait; each structure of the tuple implementation is called in order until some returns Ok() or all failed (see the tuple implementation). In the xcm/xcm-builder/src/barriers.rs file, many generic implementations can be found that can be combined to apply a custom pricing policy. More information can be found in the report, as the default configuration of barriers for the parachain template was audited.The IsTeleporter and IsReserve settings are crucial for the XCM executor to know which origins can be trusted to accept teleports and reserve transfers from. They implement the FilterAssetLocation trait and can be amalgamated into tuples. They are used in the ReceiveTeleportedAsset and ReserveAssetDeposited instruction execution code and are used as an assertion just before the ex nihilo creation of assets on the destination system. This creation process is why you have to make sure you trust the sender to have reserved or burnt the assets on the other side, i.e. properly executing the original XCM instructions.Weighers are used both in the XCM pallet and in the XCM executor configuration. For example, they will be used to weigh a newly set error or appendix handler in the executor or to adjust the weight of a call to the teleport or reserve extrinsic in the XCM pallet. They implement the WeightBounds<Call> trait. A badly implemented weigher could provide free or cheap execution that could be exploited via XCM messages. The weigher implementation in the XCM builder file is straightforward for most of the instructions, it adds a constant base weight by instruction, only Transact, SetErrorHandler and SetAppendix are a bit more sophisticated because they themselves include nested instructions or dispatchable calls.One last thing to check is the sender, or router. It must implement the SendXcm trait, which is basically the technical implementation method that takes the destination and the XCM message. Multiple senders might be combined by creating a tuple. In the Polkadot runtime, at the time of the audit, the only sender was ChildParachainRouter that sent the message to a parachain by depositing it into the downward message passing (DMP) queue of the appropriate parachain.The audit was carried out by two security engineers for a duration of 50 days. It was performed respectively on Polkadot runtime v0.9.13 and Cumulus v6.0.0. It did not uncover important security issues. Yet, the report discusses and describes the underlying security mechanisms of multiple XCM components. It can thus be interesting for anyone to dig into XCM inner workings.The full audit report can be found here:Configuring XCM should be done in a very preserving manner by denying all by default. Then, only in a discretionary way, chains should enable some messages to pass through. Most of the XCM asset transfer security boils down to deciding who is trusted as a reserve or as a teleport origin. This needs to be done with care because, when receiving assets, there are currently no mechanisms to ensure they have been properly locked/burnt on the originating chain. In this setting, the trust is shared and both chains assume the other one to behave well.Cross-chain communication protocols are developing fast, and most of them are in their infancy in terms of usage and potential use-cases. In Polkadot, XCMv2 is already deployed on the relay chain. Furthermore, the next version, XCMv3 [8], is already under development to bring new features and improve the current version.We would like to thank Parity for making this assessment possible and for their responsiveness during the audit.If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</Call></OuterOrigin></T></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Parity Tech mandated Quarkslab to audit XCM version 2 (XCMv2), a cross consensus communication mechanism. This messaging protocol is a cornerstone of the Polkadot ecosystem as it enables communications between chains on a network. This blog post summarizes few security aspects related to this technology and its implementation. The full audit report is available in PDF format at the end of this article.]]></summary></entry><entry><title type="html">Audit of the MimbleWimble Integration Inside Litecoin - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2022/audit-of-the-mimblewimble-integration-inside-litecoin-quarkslabs-blog/" rel="alternate" type="text/html" title="Audit of the MimbleWimble Integration Inside Litecoin - Quarkslab’s blog"/><published>2022-01-13T00:00:00+00:00</published><updated>2022-01-13T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2022/audit-of-the-mimblewimble-integration-inside-litecoin---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2022/audit-of-the-mimblewimble-integration-inside-litecoin-quarkslabs-blog/"><![CDATA[<p>The Litecoin Foundation mandated Quarkslab to audit the implementation of the MimbleWimble protocol in the Litecoin blockchain. This protocol acts as a sidechain in which privacy of the transactions is improved compared to the privacy on the classical chain.Since mid-2016 and the publication of the whitepapers about the MimbleWimble protocol (MW) [1] [2], the interest has grown as it offers a stronger transaction anonymity and a better network scalability. The name is inspired by a Harry Potter tongue-tying curse [3]. There is some literature about MW [4] and its underlying concepts like Scriptless scripts [5], [6]. Two implementations were proposed to make MW practicable in dedicated blockchains: GRiN and Beam.The Litecoin foundation has decided to integrate such privacy features in Litecoin. However, Litecoin is a UTXO-based blockchain while the MW inner-workings are radically different. MW operations can nonetheless be integrated to a sidechain. Among existing sidechains for Bitcoin-based blockchain we can denote Lightning Network (LN) or Liquid. While LN is designed to scale-up the underlying chain, MWEB (MimbleWimble Extension Block) focuses primarily on privacy purposes.The sidechain must adopt its own verification steps and the mainchain must verify that transactions to the sidechain are valid. The MWEB project [7] is expected to be deployed in the upcoming months.The audit of the use of MW inside Litecoin was performed for a total duration of 45 days by two engineers (22.5 days per auditor). As usual, an audit does not prove the code to be bug-free but reflects the understanding and attack scenarios that can be put in practice in the allotted time. The focus was given on the implementation of the LIPs (Litecoin Improvement Proposals) describing the MW transactions validation and the interactions between the two chains. The full report of the assessment can be found here.Instead of modifying Litecoin specifically for MW, the Litecoin foundation proposed in LIP002 [8] a general framework for implementing pegged sidechains [10] that could be activated through the standard BIP009 miner signaling mechanism of Bitcoin [9]. The two-way peg is the mechanism enabling locking coins on one side and releasing them on the other side and vice versa. That process ensure the 1-1 mapping of coins between the two chains.In this mechanism, UTXOs can be pegged into the sidechain to make them usable on it or pegged-out to leave the sidechain and be released on the canonical chain. Such proposal requires the following two main components:an Integrating Transaction (ExtTxn) which is a dedicated transaction in the canonical chain performing peg-in, peg-out transactions with the sidechain. That transaction created by the miner is specifically the last of the block.an Extension Address (ExtAddr), an anyone-can-spend bech32 address acting as an interface with the extension block (EB). As such, at each block the address contains exactly the number of coins in the sidechain. Coins are pegged in this address and pegged out from that address. It changes at each block.An EB update might also define additional consensus rules in order to accept a canonical block and its EB counterpart. These rules can only be more restrictive. In this framework, the miner is in charge of mining the whole set of blocks (canonical and EB). As part of the mining process the miner have to create ExtTxn linking both blocks.MWEB (MimbleWimble Extension Block) is the integration of MW in Litecoin using the EB framework proposal. The integration is described in LIP003 [11] and further defines the handling of coins on the sidechain.MW leverages Confidential Transactions [12] and Bulletproof [13] against Confidential Transactions alone or ZK-STARKs or ZK-SNARKs as it provides the most compact transactions and avoid a trusted setup.Implemented as an EB, ExtTx are called HogEx for Hogwarts Express transaction, and ExtAddr are called HogAddr for Hogwarts Addresses. The later will use a new version (the version 9) of witness programs. The following schema shows a simplified view of a LTC block and its MW counterpart.As shown on the figure, a peg-in transaction by a user indicates a commitment as recipient in the MWEB world (recorded in the MWEB block as a kernel). In the HogEx transaction, the miner spends those coins to the new HogAddr. Similarly, coins can be pegged-out by creating a peg-out on the MWEB side which indicates an amount and the LTC address to send coins to. This kernel is materialized on the canonical side as outputs from the previous HogAddr.Transactions within the MWEB world (MW-to-MW) do not look like traditional Litecoin transactions but follow the same workflow. Indeed they will enter the mempool [14] but will be mined into the MWEB block rather than the LTC block.More details about the MW Extension Block integration and modifications implied on the Litecoin codebase can be found in Section 4 of the report [15].MW extensively uses Pedersen commitments [16], Schnorr signatures [17] [18] and Bulletproofs [13]. One of the basis operations of all these schemes is based on secp256k1 [19], which allows to reuse parts of the underlying cryptographic library already included in Litecoin. However, these schemes are not (yet) included in the core cryptographic library of Litecoin: the new primitives are included in the secp256k1-zkp directory, close to this implementation. We reviewed in particular some elements of the LIP004 [20] and the API calls through the C++ wrappers in the code.Also, some modifications were applied to the original MW protocol in order to make it more usable. For example, LIP004 [20] describes a way to make MW non-interactive, which is a way for the sender to build alone the transaction and transmit to the recipients the information, through stealth addresses [21], for spending the coins.Section 5 of the report [15] contains more information about the review of the MW cryptography inside Litecoin.Auditing blockchains raises specific security concerns but adding sidechain features brings a whole range of new potential issues, e.g., consensus issues or assets amount discrepancies between chains. Thankfully, MWEB is a synchronized sidechain as they both generate a new block in a synchronized manner which reduces the scope of issues that can arise. This notably avoids fork related issues. Assessed security aspects are twofold, the sidechain security issues and the confidentiality aspects of MW implementation. Underlying security properties that should be preserved are the following:all consensus rules (policy) already defined in Litecoin must be preserved;new consensus rules need to be more restrictive on accepted blocks in order to be accepted through a soft-fork;at all times, the sum of pegged-in coins should equal the amount of coins on the sidechain;transaction amounts on MW sidechain should remain confidential between emitter and recipient.As most blockchains, the threat model considers transaction and blocks produced by miners to be untrusted. The full audit report can be found here:Multiple low-impact defects have been identified but a more significant security issue has been identified in the block validation process which impacts the consensus. As shown on the figure, well-known Bitcoin Core functions CheckBlock, CheckTransaction, AcceptBlock or ConnectBlock find their counterparts on the sidechain code.Even if the code to perform the verification is implemented, the CheckBlock function on MW side has not been properly called from the canonical side. It theoretically enables subtle validation issues and consensus issues, allowing the acceptation of corrupted blocks. Thankfully, this issue can trivially be addressed. Issues mentioned in the report have carefully been fixed.We would like to thank the Litecoin Foundation for making this assessment possible, and David Burkett for his commitment and responsiveness all along the audit.If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The Litecoin Foundation mandated Quarkslab to audit the implementation of the MimbleWimble protocol in the Litecoin blockchain. This protocol acts as a sidechain in which privacy of the transactions is improved compared to the privacy on the classical chain.]]></summary></entry><entry><title type="html">Remote Denial-of-Service on CycloneTCP : CVE-2021-26788 - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2021/remote-denial-of-service-on-cyclonetcp-cve-2021-26788-quarkslabs-blog/" rel="alternate" type="text/html" title="Remote Denial-of-Service on CycloneTCP : CVE-2021-26788 - Quarkslab’s blog"/><published>2021-04-13T00:00:00+00:00</published><updated>2021-04-13T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2021/remote-denial-of-service-on-cyclonetcp--cve-2021-26788---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2021/remote-denial-of-service-on-cyclonetcp-cve-2021-26788-quarkslabs-blog/"><![CDATA[<p>This post is a quick vulnerability report summary for a vulnerability we found while fuzzing the TCP/IP stack CycloneTCP.CycloneTCP is a fully-featured open source IPv4/IPv6 stack for embedded systems developed by Oryx Embedded. The stack is developed in ANSI C and distributed under GPLv2 or with a commercial license. The source code is available on Oryx’s website (or a Github mirror). It supports various microprocessors, operating systems and a whole variety of protocols including HTTP, SNMP, Icecast, CoAP, MQTT, etc.While working on TCP/IP stacks for embedded devices we ended up experimenting various software testing techniques on CycloneTCP like fuzzing with Honggfuzz [1] and concolic execution with Triton [2]. The fuzzing harness we wrote only targeted a few components (IPv4/TCP without other protocols). While running a campaign, a hanging input sample was generated, that ultimately led the CycloneTCP in an infinite loop. After a quick investigation, it appeared to be a remotely triggerable Denial-of-Service vulnerability in the handling of TCP options.Affected Versions : From CycloneTCP 1.7.6 to 2.0.0 (we have not been able to check versions earlier than 1.7.6).Fixed Versions : CycloneTCP 2.0.2 and above.Impact : Remote Denial of Service.Scope : The bug happens in the default configuration, and is triggerable by any attacker that can communicate with a device using CycloneTCP as long as IPv4 and TCP are activated.Potential mitigations : If it is not possible to update the device with a fixed version of CycloneTCP, filtering inbound TCP packets with a zero length TCP option will prevent exploitation of the bug.A malformed packet can trigger an infinite loop in the function tcpGetOption() of tcp_misc.c.At line 541, the statement i += option-&gt;length; increments i with a value that is user-controlled. If the value option-&gt;length is set to 0, then the tcpGetOption() function will never exit the while loop. The length check should be performed and exit the loop if option-&gt;length value is less than 2 (as it can be seen in linux TCP stack [3]).This portion of code is executed by the main thread of the stack scheduling and processing all frames. It includes processing incoming frames from the driver up to transmitting the payload to applicative threads. This thread also performs all internal “ticks” and internal state updates. Thus, if it gets stuck, it can no longer receive incoming frames or handle data coming from applicative layers.In conclusion, by sending a TCP segment with a TCP option with a length value of zero, an attacker can put the CycloneTCP stack in an infinite loop that prevents it from processing any other packet.We proposed the following patch, which consists in a single check at the end of the while loop to properly exit it, and especially, if the option-&gt;length given in the TCP segment is an invalid one.This patch fixes the infinite loop and normally has no deleterious effects on the processing of other options.MITRE assigned the identifier CVE-2021-26788 to the vulnerability [4].Even though we believe the vulnerability cannot be used to gain further privileges on the device it can nonetheless temporarily DOS the device. To our knowledge, solely a reboot can restore the stack to a working state. We strongly encourage any user (of both the open-source and commercial version) to upgrade to the latest version (as the only other remediations are dedicated filtering rules).While we only tested a narrow portion of the whole CycloneTCP code, the tests we performed have shown the stack to be robust, well designed and trustworthy in comparison to other existing stacks. We also want to highlight the reactiveness and commitment of the Oryx team in the disclosure process.2020-10-15: Discovery of the vulnerability.2020-12-21: Email sent to Oryx Embedded.2020-12-23: Vendor requested details and PoC.2020-12-23: Technical details and PoC sent.2021-01-11: CycloneTCP v2.0.2 release fixes the issue. Vendor sends release notes and security advisory.2021-01-13: Email sent to coordinate disclosure.2021-01-19: Reply from vendor suggesting a conference call in February.2021-02-02: Email to vendor proposing dates &amp; times for a conference call to coordinate disclosure.2021-02-02: CVE ID requested via web form at https://cve.mitre.org, automated reply received.2021-02-17: Email sent to MITRE asking for update on CVE ID request.2021-03-04: Email sent to MITRE asking for update on CVE ID request.2021-03-08: CVE-2021-26788 assigned by MITRE.2021-03-09: Email sent to Oryx Embedded to coordinate disclosure date. Conference call scheduled for 2021-03-16.2021-03-16: Conference call with the vendor. Publication date set to April 8th, 90 days after initial report.2021-04-13: Vulnerability details disclosed.If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This post is a quick vulnerability report summary for a vulnerability we found while fuzzing the TCP/IP stack CycloneTCP.]]></summary></entry><entry><title type="html">An Experimental Study of Different Binary Exporters - Quarkslab’s blog</title><link href="https://robindavid.github.io/blog/2019/an-experimental-study-of-different-binary-exporters-quarkslabs-blog/" rel="alternate" type="text/html" title="An Experimental Study of Different Binary Exporters - Quarkslab’s blog"/><published>2019-09-24T00:00:00+00:00</published><updated>2019-09-24T00:00:00+00:00</updated><id>https://robindavid.github.io/blog/2019/an-experimental-study-of-different-binary-exporters---quarkslabs-blog</id><content type="html" xml:base="https://robindavid.github.io/blog/2019/an-experimental-study-of-different-binary-exporters-quarkslabs-blog/"><![CDATA[<p>This blog post presents a comparison between various disassembled binary exporters.All the tools presented in this blog post have been tested in accordance with the knowledge we had of them. We do not claim at all that our results are an accurate view of the state of the tools, and we probably missed features we did not know about. The figures should be seen as indicators and not as ground truth.Analyzing binaries programs often requires to disassemble them. The two most famous tools for this task are IDA Pro and the newer one from the NSA Ghidra. Even if really powerful, these tools are inadequate for running custom analyses on a disassembled binary or on multiple binaries at the same time. If the disassembler is not needed anymore, why bothering to keep it open and running in the background? This is actually costly, as each instance may eat up to a few hundreds of megabytes in RAM. The only necessary element is an export of the disassembled binary and this blog post presents an overview of the different exporters and disassemblers available.For the rest of this article, an export of a binary is defined as a file which stores various information about the program. These data range from meta information (format, architecture, compilers identification) to more specific elements on the disassembled code itself (instructions, mnemonics) and intelligence gathered by the disassembler (x-references, symbols).The first step to export a disassembled binary is to disassemble it. Numerous tools exist for this task, the most famous one being IDA, a commercial tool by HexRays. During the last years, other tools have been released (Binary Ninja, radare, Ghidra) with different ranges of features and prices. While this blog post does not conduct a complete review of all the existing tools, nor pretends to as it would be slippery, we still wanted to have a more informed opinion on the different options.The table below lists some important tools to disassemble a complete binary and some of their features.As seen in the table above, where only active projects are listed, there is a broad range of tools available. It was not possible to compare all the binary disassembly tools, as our time was limited. We thus elected not to include the following:Binary Ninja: we had no license for the tool;McSema: it relies on IDA to perform the disassembling;BAP: the python bindings are using a client/server model that is not really practical for our needs;Pharos: tuned to be used for C++ disassembly;Macaw: supports a limited set of architecture.Note: Even if these tools have been left aside because they did not seem to fit our needs, they are nice pieces of engineering. We still encourage everyone to have a look at them. [1]To test the performances of the disassemblers, the three following programs were used, classified in three categories, small, medium and large. The selected binaries are:Small: elf-Linux-x64-bash (~900KB) ELF file for x86-64 (source: Linux)Medium: delta_generator (17MB) ELF file for x86 (source: Android Open Source Project)Large: llvm-opt (34MB) ELF file for x86-64 (source: LLVM)These programs were selected at random from programs available on our computers at the time of the tests. They are not supposed to have any outstanding features, just regular programs coming from widely used open-source projects.All tests were run on a Dell XPS 15 with an Intel® Core™ i7-6700HQ CPU @ 2.60GHz with an SSD and 16 Go RAM running Debian 10 (Buster).Some notes on the table:These disassembly figures should be handled with care as there is no ground truth results (in terms of instructions/functions count). Nonetheless, IDA/Ghidra results can be considered as a close approximation of the right results.angr is a complex tool which performs control flow analysis through code emulation to correctly disassemble a binary. However, this implies to have some stubs [2] written to perform the emulation. While a lot of the necessary stubs are already available, some are still missing (hence the errors shown). To our knowledge, there is no method in angr to disassemble a binary without generating a CFG (see #1116).Miasm needs the entry points of a program to start disassembling. It performs the disassembly from one entry point recursively until no instruction is found. The two functions found by the tool are actually the two entry points we specified (_start and main) [3].The x86_64 version of the delta_generator program was used for ddisasm because x86 is not (yet) supported by the tool.JEB uses a slightly broader notion of functions where every instruction is assigned to a function (while IDA leaves orphaned instructions). New functions are also created for exception handlers and per switch target if it does not succeed in reconstructing it. These are factors which explain the difference in function numbers. In terms of retrieved instruction numbers it performs as well as IDA and Ghidra while being more similar to Ghidra in terms of speed.Both IDA and Ghidra stand out for the number of instructions and functions retrieved, with a little advantage to IDA for its disassembly speed. These results are not surprising since IDA and Ghidra are huge players with decades of experience.The following step is to export the disassembled program into a standalone file. The goal is to close the disassembler after the initial disassembly step, as its features are not needed anymore.The list of exporters available for the tools tested in the first section is shown below.Ghidra provides an IDA plugin to generate an XML file (and a raw data file) so the user can import them in Ghidra.The widely used tool BinDiff uses BinExport, a Protobuf generated file, exported from IDA as a basis to perform its diffing. One of the authors of BinExport has started a port of the exporting feature on Ghidra (the proof-of-concept is available in his personal project on GitHub and worked really nicely so far).ddisasm is able to parse a binary file and export a lot of information via a Protobuf file. The ultimate goal of the toolchain developed by GrammaTech is to do binary rewriting [4]. As a consequence, the exported features focus on the sole information useful for this task. This represents only a subset of all the available information.We also found other exporters that were left out of this study:Diaphora: The tool exports a binary to a SQLite database and is written in Python. A preliminary study has shown that the sqlite file is much larger (around 4-6 times) than the i64 and thus not compact enough for our needs.YaCo: Plugin developed by the Direction Générale de l’Armement (DGA) for the YaTools suite. It does not export any information below the granularity of basic blocks (and only a hash of them). However, it is worth noticing as it is the only tool generating a FlatBuffers file.bnida: Plugin used to port a project from IDA to Binary Ninja. It exports to a JSON file and is written in Python. It does not export any data on the content of the functions (just their names and address) nor below this granularity.JEB: JEB has a built-in exporter that exports the disassembled (and decompiled) code as C-code in files. While interesting, this approach is not really suitable for our purposes.The table below details the various information exported by the different exporters selected. The results were gathered by analyzing the description of the protocol and actual exported files.To improve readability, explanations for ambiguous results (orange tildes) are provided as tooltips.Important notes:The goals of the different exporters are not identical, so they do not export the same type of information from a binary. While BinExport was designed to be a part of a diffing engine, ddisasm was designed to be a part of binary rewriting toolchain.In the Ghidra-XML column, when the exported information varies between the IDA and Ghidra implementations, those differences are noted.Two main strategies exist for exporters. The first one is to export disassembled instructions with information on their content (mnemonic, operands, expressions inside the operands). Using this strategy, the export itself is self-contained and no other tool is required to analyze it. The second strategy is to export only the raw bytes (of the instructions) themselves and leave the remaining disassembly work to another disassembler (e.g capstone). An export using this strategy will be more compact, but at the price of needing a helping tool to understand the content of the export. The choice of the strategy obviously depends on the final objective of the tool. It makes sense for Ghidra not to export disassembled instructions because they have their own disassembler, and for BinExport to export everything because BinDiff should be autonomous (and as fast as possible).This sections aims to compare with more details the exporters found for IDA and Ghidra. The results of the first section of this article comforted us to only consider those two disassemblers as they were more accurate.We are also interested in comparing the performance of the built-in exporter of Ghidra against the plugin they offer for IDA. However, we choose not to include the experimental port of BinExport for Ghidra because it is still a work in progress and its performances are below the ones from IDA’s version while exporting the same features.For the rest of the benchmarks, we gathered a dataset of various binaries coming from different sources. While our dataset is not exhaustive, it tries to mimic the diversity of programs a reverser could encounter. It gathers binaries of various architectures, files formats, size and bitness. The sources used are listed below [5] :binary-samples: A test suite for binary analysis tools made by Jonathan SalwanAOSP (Android Open Source Project): An open source operating system for mobile devicesLLVM: The compiler infrastructure projectThe graph above shows the number of instructions per program in the dataset. If most of our test suite is made of programs with less than a million instructions, a few large binaries were also included, to better understand how the exporters and disassemblers scaled. As we need to plot large ranges of values in the same graph, most of the curves looks flat for the first points. [6]The first metrics we were interested in is the disassembly time, defined as the duration of the automatic analysis. We knew that IDA was faster than Ghidra, but we wanted to measure to what extent.The results are impressive, Ghidra is much slower than IDA (up to 13 times slower for large binaries). Even if the disassembly step is a one time process, the performances of Ghidra are problematic for scalability. Nevertheless, it should be noted that the results are biased, because Ghidra performs an additional decompilation step.The first section helped us to draw an overview of the available exporters. Another interesting metrics is the export time for the following disassemblers/exporters pairs:IDA + BinExportIDA + Ghidra XMLGhidra + XMLWe chose to keep only those exporters because they were running on the disassemblers we selected, and had an interesting set of exported features. They also had a good support for Ghidra, and BinDiff has been used for years in the community without issues. We may also note that they use different exporting strategies: Ghidra does not export any information on instructions while BinExport decomposes every operand of each instruction and exports them.The export size of a program is far greater than the program itself for both tools. While BinExport produces a single Protobuf file, Ghidra generates two files, one XML with all the information and a raw byte file containing all the code of the exported binary. The figures on the graph represent the sum of the size of these two files.We observe that the size of the export for BinExport and XML is roughly the same. However, BinExport exports a lot more information on the binary than Ghidra. Remember that Ghidra does not export any information on the instructions themselves neither on the basic blocks besides their contents (i.e. raw bytes). The sizes of the exported files remain equivalent because of optimizations made by BinExport: the format is specifically designed for compactness (e.g. there is an extensive usage of deduplications tables) and the export file uses a binary serialization protocol, namely Protobuf. This will be further discussed in the next section.The table above also includes the sizes of the database generated by IDA, the i64 file, which is much larger than any of the exported file considered in this study.To summarize the results from the previous tests, we plot hereafter a graph explaining the time spent in the three phases of the export process:Disassembly phase: disassembling the binaryExport phase: generating the export filesDeserialization / Loading phase: Importing the exported file in PythonThis graph shows that the deserialization time can to become non-negligeable with the Protobuf format for large binaries (here mdbook). This observation led us to the next section which explores various binary serialization formats to find which one is the more suitable for our needs.Numerous formats exist [7] for serialization because not all usages (e.g persistent storage, RPC communication, data transfer, …) require the same set of features. One may want to have the data stored in a “human-readable” way (i.e as text), have a fast-access time, or a compact storage size. For program serialization, we need a trade-off between a compact disk usage, a reasonable deserialization time and a low memory footprint. Since a readable format is not needed and disk usage is a concern, binary serialization formats seemed more appropriate, as opposed to text formats (e.g. JSON, XML).In this section, we will focus on three formats used for binary serialization:Protobuf: A format developed (and extensively used) by Google for serializing structured data.FlatBuffers: Another format developed by Google to serialize data. Mostly used for performance critical applications.Cap’n Proto: A format developed by Kenton Varda (tech lead of Protobuf while he was working at Google) for Sandstorm.All these formats use a custom schema definition language to explain how the data will be formatted on the wire. Even if this blog post does not intend to be a crash course on data serialization, nor a tutorial on how to write a schema for the three protocols, the syntax of a basic message is shown below.The main difference between these formats is how they store data on the wire. Protobuf, the oldest one, uses an encoding/packing step which transforms the input on the wire. This allows Protobuf to be more compact because the encoding step reduces the amount of bytes needed to store an object (see Encoding in Protobuf documentation). However, both FlatBuffers and Cap’n Proto use a ‘zero-copy’ strategy, meaning that the data on the wire is structured the same way as it is in the memory. The main advantage of this technique is to nullify the time needed to decode the object because no decoding step is performed.Another huge difference between FlatBuffers/Cap’n Proto and Protobuf is the ability to perform random access reads (the ability to read a specific part of the message without reading the whole message before). With Protobuf this is not possible because the message needs to be parsed upfront (and memory allocated). However, both FlatBuffers and Cap’n Proto implement this feature using pointers, allowing fast access to part of the message.Allocation (i.e how to write message) has to be done bottom-up for FlatBuffers because a message must be finished before another one is started. This limitation does not apply to Protobuf (because all the message is written at the end) and Cap’n Proto (because the size of an object is known when allocated).The final difference we will go through is how unset fields (i.e. fields with no values for this specific message) are stored on the wire. Both Protobuf and FlatBuffers do not allocate them while Cap’n Proto still do. This leads to a waste of space for Cap’n Proto.For these benchmarks, we translated the BinExport Protobuf into a FlatBuffers and a Cap’n Proto schema. The translation was done manually for Cap’n Proto and using the option –proto of flatc for FlatBuffers (plus some minor revisions). We do not pretend to have fully optimized the new schemes using all the features of the two serializations formats but believe this still leads to an informative comparison.First, we want to compare how big the exported files are compared to the binaries themselves. This size is represented by the dashed line and is linear (y=x \def\pelican{\textrm{pelican}^2}</p> <p>y=x).We see that the size of the exported file grows non-linearly with the size of the binary. The following graph shows the ratio between the size of the exported file and the size of the binary.We see that Protobuf is much more compact than the two others (the encoding step is crucial for this part) and the ratio skyrockets for specific binaries. There is still room for improvement in the export size for the two other protocols, mostly by having a better understanding of the ranges of the different values. With Protobuf, one may declare every integers as 64-bits wide integers, the serialization algorithm will only write on the wire the varint encoded value of the number (a reduction up to a scale factor of 8 for the 127 first values). However, with Cap’n Proto and FlatBuffers, the value would need to be 64 bits long anyway.Another interesting point to study is how much memory is used for loading the serialized file in Python. (Note: using the memory_profiler [8] module to retrieve memory usage.)As expected, the memory needed to load the export of a binary is much more important for Protobuf. For example, for llvm-opt the Protobuf file is around 150MB but the loading takes around 1.8 Go of RAM.The last metrics we want to consider is how much time is needed to load an export file in Python from the three files format.As expected, the Protobuf format takes a lot of time to be deserialized. Cap’n Proto and FlatBuffers have similar performances, mostly because they are based on the same patterns.We could have reduced the size of the exported file for Cap’n Proto by applying their ‘packed’ algorithm. However, this removes the interesting property of having a ‘zero-copy’ protocol. More experiments are still needed to understand if this would be a better option than Protobuf.Compressing the exported file using well-known algorithms could also be a viable strategy for Cap’n Proto and FlatBuffers as it would also reduce the size of the exported file. However, this option adds some time upfront, as it requires to decompress the file before using it. It is not applicable to Protobuf because the format is already compact.Exporting as many data as possible from a binary is interesting not in itself but as a basis for other applications, like features extraction for machine learning algorithms, graph traversing algorithms, or fast access to functions / blocks / instructions based on user defined criteria.This blog post explored different options to export a disassembled program from a disassembler using available exporters. To the best of our knowledge, the most complete exporter available is BinExport as it exports a lot of information while remaining compact thanks to the serialization format used, Protobuf. Nonetheless, there is still room for improvement for binary exporters as none of the explored solutions answered all our scalability needs.09.25.19 : Update the results for radare using the last version (from 3.2.1 to 4.0.0)10.30.19 : Add the results for JEB, a disassembler (and decompiler) by PNFSoftware.If you would like to learn more about our security audits and explore how we can help you, get in touch with us! Back to top</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This blog post presents a comparison between various disassembled binary exporters.]]></summary></entry></feed>